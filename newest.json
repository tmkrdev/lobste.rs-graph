[{"short_id":"kcnqgp","created_at":"2026-01-30T12:06:26.000-06:00","title":"Amdahl’s law and agentic coding","url":"https://evnm.substack.com/p/amdahls-law-and-agentic-coding","score":1,"flags":0,"comment_count":0,"description":"","description_plain":"","submitter_user":"evnm","user_is_author":true,"tags":["vibecoding"],"short_id_url":"https://lobste.rs/s/kcnqgp","comments_url":"https://lobste.rs/s/kcnqgp/amdahl_s_law_agentic_coding"},{"short_id":"5rjh8v","created_at":"2026-01-30T12:05:46.000-06:00","title":"Subtypes and status-dependent data: pure relational approach","url":"https://minimalmodeling.substack.com/p/subtypes-and-status-dependent-data","score":2,"flags":0,"comment_count":0,"description":"","description_plain":"","submitter_user":"squadette","user_is_author":true,"tags":["databases"],"short_id_url":"https://lobste.rs/s/5rjh8v","comments_url":"https://lobste.rs/s/5rjh8v/subtypes_status_dependent_data_pure"},{"short_id":"bhoyfm","created_at":"2026-01-30T11:37:23.000-06:00","title":"Ingress NGINX: Statement from the Kubernetes Steering and Security Response Committees","url":"https://kubernetes.io/blog/2026/01/29/ingress-nginx-statement/","score":5,"flags":0,"comment_count":1,"description":"","description_plain":"","submitter_user":"Corbin","user_is_author":false,"tags":["devops","distributed","release"],"short_id_url":"https://lobste.rs/s/bhoyfm","comments_url":"https://lobste.rs/s/bhoyfm/ingress_nginx_statement_from_kubernetes"},{"short_id":"w8d5zt","created_at":"2026-01-30T11:09:17.000-06:00","title":"Paty: The most human-like AI agent you'll ever use","url":"https://github.com/gjtorikian/paty","score":1,"flags":1,"comment_count":0,"description":"","description_plain":"","submitter_user":"gjtorikian","user_is_author":true,"tags":["ai","satire"],"short_id_url":"https://lobste.rs/s/w8d5zt","comments_url":"https://lobste.rs/s/w8d5zt/paty_most_human_like_ai_agent_you_ll_ever"},{"short_id":"tnjpon","created_at":"2026-01-30T10:52:17.000-06:00","title":"Announcing skills on Tessl: the package manager for agent skills","url":"https://tessl.io/blog/skills-are-software-and-they-need-a-lifecycle-introducing-skills-on-tessl/","score":1,"flags":0,"comment_count":0,"description":"","description_plain":"","submitter_user":"popey","user_is_author":false,"tags":["vibecoding"],"short_id_url":"https://lobste.rs/s/tnjpon","comments_url":"https://lobste.rs/s/tnjpon/announcing_skills_on_tessl_package"},{"short_id":"5piakr","created_at":"2026-01-30T10:47:59.000-06:00","title":"How we interfaced single-threaded C++ with multi-threaded Rust","url":"https://antithesis.com/blog/2026/rust_cpp/","score":1,"flags":0,"comment_count":0,"description":"","description_plain":"","submitter_user":"LucasPickering","user_is_author":false,"tags":["c++","rust"],"short_id_url":"https://lobste.rs/s/5piakr","comments_url":"https://lobste.rs/s/5piakr/how_we_interfaced_single_threaded_c_with"},{"short_id":"schhxu","created_at":"2026-01-30T09:59:33.000-06:00","title":"A \"Pure Go\" Linux environment, ported by Claude, inspired by Fabrice Bellard","url":"https://www.jtolio.com/2026/01/tinyemu-go/","score":1,"flags":2,"comment_count":0,"description":"","description_plain":"","submitter_user":"carlana","user_is_author":false,"tags":["go","linux","vibecoding"],"short_id_url":"https://lobste.rs/s/schhxu","comments_url":"https://lobste.rs/s/schhxu/pure_go_linux_environment_ported_by"},{"short_id":"kfo7mc","created_at":"2026-01-30T09:36:45.000-06:00","title":"Building Breakwater with AI","url":"https://www.bencurtis.com/2026/01/building-breakwater-with-ai/","score":1,"flags":0,"comment_count":1,"description":"","description_plain":"","submitter_user":"bencurtis","user_is_author":true,"tags":["vibecoding","web"],"short_id_url":"https://lobste.rs/s/kfo7mc","comments_url":"https://lobste.rs/s/kfo7mc/building_breakwater_with_ai"},{"short_id":"zvcae4","created_at":"2026-01-30T07:57:02.000-06:00","title":"cli-stash: TUI tool to save and recall shell commands with fuzzy search","url":"https://github.com/itcaat/cli-stash","score":4,"flags":0,"comment_count":2,"description":"\u003cp\u003eBuilt this to solve my own problem of forgetting complex commands. Instead of digging through shell history or notes, cli-stash lets you:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBrowse shell history and save commands (Ctrl+A)\u003c/li\u003e\n\u003cli\u003eFuzzy search through saved commands\u003c/li\u003e\n\u003cli\u003eSelect and insert directly into terminal prompt\u003c/li\u003e\n\u003cli\u003eAuto-sort by usage frequency\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBuilt with Go and Bubble Tea. Feedback welcome.\u003c/p\u003e\n","description_plain":"Built this to solve my own problem of forgetting complex commands. Instead of digging through shell history or notes, cli-stash lets you:\r\n- Browse shell history and save commands (Ctrl+A)\r\n- Fuzzy search through saved commands\r\n- Select and insert directly into terminal prompt\r\n- Auto-sort by usage frequency\r\n\r\nBuilt with Go and Bubble Tea. Feedback welcome.","submitter_user":"itcaat","user_is_author":true,"tags":["linux","mac"],"short_id_url":"https://lobste.rs/s/zvcae4","comments_url":"https://lobste.rs/s/zvcae4/cli_stash_tui_tool_save_recall_shell"},{"short_id":"clng5d","created_at":"2026-01-30T07:26:50.000-06:00","title":"Full AI Suite for LispE: llama.cpp, tiktoken, MLX and PyTorch","url":"","score":2,"flags":1,"comment_count":0,"description":"\u003cp\u003eI have presented \u003ca href=\"https://github.com/naver/lispe\" rel=\"ugc\"\u003eLispE\u003c/a\u003e a few times in this forum. LispE is an Open Source version of Lisp, which offers a wide range of features, which are seldom found in other Lisps.\u003cbr\u003e\nI have always wanted to push LispE beyond a simple niche language, so I have implemented 4 new libraries:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/naver/lispe/tree/master/lispetiktoken\" rel=\"ugc\"\u003elispe_tiktoken\u003c/a\u003e (Openai tokenizer)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/naver/lispe/tree/master/lispegguf\" rel=\"ugc\"\u003elispe_gguf\u003c/a\u003e (encapsulation of llama.cpp)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/naver/lispe/tree/master/lispemlx\" rel=\"ugc\"\u003elispe_mlx\u003c/a\u003e (Mac OS's own ML library encapsulation)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/naver/lispe/tree/master/lispetorch\" rel=\"ugc\"\u003elispe_torch\u003c/a\u003e (An encapsulation of torch::tensor and SentencePiece, based on PyTorch internal C++ library)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eI provide the full binaries of these libraries only for Mac OS (see \u003ca href=\"https://github.com/naver/lispe/releases/tag/LispE_MacOS_Tar\" rel=\"ugc\"\u003eMac Binaries\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eWhat is really interesting is that the performance is usually better and faster than Python. For instance, I provide a program to fine-tune a model with a LoRA adapter, and the performance on my Mac is 35% faster than the comparable Python program.\u003c/p\u003e\n\u003cp\u003eIt is possible to load a HuggingFace model, to load its tokenizer and to execute inferences directly in LispE. You can also load GGUF models (the llama.cpp format) and run inference directly within LispE. You can download models from \u003ca href=\"https://ollama.com\" rel=\"ugc\"\u003eOllama\u003c/a\u003e or \u003ca href=\"https://lmstudio.ai\" rel=\"ugc\"\u003eLM-Studio\u003c/a\u003e, which are fully compatible with lispe_gguf.\u003c/p\u003e\n\u003cp\u003eThe MLX library is a full fledged implementation of the MLX set of instructions on Mac OS. I have provided some programs to do inference with specific MLX compiled models. The performance is on par and often better than Python. I usually download the model from \u003ca href=\"https://lmstudio.ai\" rel=\"ugc\"\u003eLM-Studio\u003c/a\u003e, with the MLX flag on.\u003c/p\u003e\n\u003cp\u003eThe whole libraries should compile on Linux, but if you have any problems, feel free to open an issue.\u003c/p\u003e\n\u003cp\u003eNote: MLX \u003cem\u003eis only available for Mac OS.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eHere is an example of how to load and execute a GGUF model:\u003c/p\u003e\n\u003cpre lang=\"Lisp\"\u003e\u003ccode\u003e    ; Test with standard Q8_0 model\r\n    (use 'lispe_gguf)\r\n    \r\n    (println \"=== GGUF Test with Qwen2-Math Q8_0 ===\\n\")\r\n    \r\n    (setq model-path \"/Users/user/.lmstudio/models/lmstudio-community/Qwen2-Math-1.5B-Instruct-GGUF/Qwen2-Math-1.5B-Instruct-Q8_0.gguf\")\r\n    \r\n    (println \"File:\" model-path)\r\n    (println \"\")\r\n    (println \"Test 1: Loading model...\")\r\n    \r\n    ; Configuration: uses GPU by default (n_gpu_layers=99)\r\n    ; For CPU only, use: {\"n_gpu_layers\":0}\r\n    (setq model\r\n       (gguf_load model-path\r\n          {\"n_ctx\":4096\r\n             \"cache_type_k\":\"q8_0\"\r\n             \"cache_type_v\":\"q8_0\"\r\n          }\r\n       )\r\n    )\r\n    \r\n    ; 2. Generate text only if model is loaded\r\n    (ncheck (not (nullp model))\r\n       (println \"ERROR: Model could not be loaded\")\r\n       (println \"Generating text...\")\r\n       (setq prompt \"Hello, can you explain what functional programming is?\")\r\n       ; Direct generation with text prompt\r\n       (println \"\\nPrompt:\" prompt)\r\n       (println \"\\nResponse:\")\r\n       (setq result (gguf_generate model prompt {\"max_tokens\":2000 \"temperature\":0.8 \"repeat_penalty\":1.2 \"repeat_last_n\":128}))\r\n       (println)\r\n       (println \"-----------------------------------\")\r\n       (println (gguf_detokenize model result)))\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cstrong\u003eWhy is it different?\u003c/strong\u003e\n\u003cp\u003eOne of the first important things to understand is that when you are using Python, most of the underlying libraries are implemented in C++. This is the case for MLX, PyTorch and llama.cpp. Python requires a heavy API to communicate with these libraries, with constant translations between the different data structures. Furthermore, these APIs are usually pretty complex to modify and to transform, which explains why there is a year-long backlog of work at the PyTorch Foundation.\u003c/p\u003e\n\u003cp\u003eIn the case of LispE, the API is incredibly simple and thin, which means that it is possible to tackle a problem \u003cem\u003eeither as LispE code\u003c/em\u003e or when speed is required \u003cem\u003eat the level of the C++\u003c/em\u003e. In other words, LispE provides something unique: \u003cem\u003ea way to implement and handle AI both through the interpreter or through the library.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThis is how you define a LispE function and you associate this function with its C++ implementation:\u003c/p\u003e\n\u003cpre lang=\"C++\"\u003e\u003ccode\u003e        lisp-\u0026gt;extension(\"deflib gguf_load(filepath (config))\",\r\n                        new Lispe_gguf(gguf_action_load_model));\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOn the one hand, you define the signature of the library function, which you associate with an instance of a C++ object. Once you've understood the trick, it takes about 1/2 hours to implement your own LispE functions. Compared to Python, there is no need to handle the life cycle of the arguments, this is done for you.\u003c/p\u003e\n\u003cpre lang=\"C++\"\u003e\u003ccode\u003e        Element* config_elem = lisp-\u0026gt;get_variable(\"config\");\r\n        string filepath = lisp-\u0026gt;get_variable(\"filepath\")-\u0026gt;toString(lisp);\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe name of your arguments is \u003cem\u003ethe way to get their values on top of the execution stack.\u003c/em\u003e In other words, LispE handles the whole life cycle itself, no need for PyDECREF or other horrible macros.\u003c/p\u003e\n\u003cstrong\u003eLispE is close to the metal\u003c/strong\u003e\n\u003cp\u003eOne of the most striking features of LispE is that it is very close to the metal in the sense that a LispE program is compiled as a tree of C++ instances. Contrary to Python, where the code in the libraries executes outside of the VM, LispE doesn't make any difference between an object created in the interpreter or into a library, they both derive from the \u003cem\u003eElement\u003c/em\u003e class and are handled in the same way. You don't need to leave the interpreter to execute code, because the interpreter instances are indistinguishable from the library instances. The result is that LispE is often much faster than Python, while proposing one of the simplest APIs to create libraries around.\u003c/p\u003e\n\u003cstrong\u003eWhat is next?\u003c/strong\u003e\n\u003cp\u003eThe \u003cem\u003elispe_torch\u003c/em\u003e library is still a work in progress, for instance MoE is not implemented yet in the \u003cem\u003eforward.\u003c/em\u003e In the case of \u003cem\u003etiktoken, gguf and MLX\u003c/em\u003e, the libraries are pretty extensive and should provide the necessary bricks to implement better models.\u003c/p\u003e\n","description_plain":"I have presented [LispE](https://github.com/naver/lispe) a few times in this forum. LispE is an Open Source version of Lisp, which offers a wide range of features, which are seldom found in other Lisps.  \r\nI have always wanted to push LispE beyond a simple niche language, so I have implemented 4 new libraries:\r\n\r\n1. [lispe\\_tiktoken](https://github.com/naver/lispe/tree/master/lispetiktoken) (Openai tokenizer)\r\n2. [lispe\\_gguf](https://github.com/naver/lispe/tree/master/lispegguf) (encapsulation of llama.cpp)\r\n3. [lispe\\_mlx](https://github.com/naver/lispe/tree/master/lispemlx) (Mac OS's own ML library encapsulation)\r\n4. [lispe\\_torch](https://github.com/naver/lispe/tree/master/lispetorch) (An encapsulation of torch::tensor and SentencePiece, based on PyTorch internal C++ library)\r\n\r\nI provide the full binaries of these libraries only for Mac OS (see [Mac Binaries](https://github.com/naver/lispe/releases/tag/LispE_MacOS_Tar)).\r\n\r\nWhat is really interesting is that the performance is usually better and faster than Python. For instance, I provide a program to fine-tune a model with a LoRA adapter, and the performance on my Mac is 35% faster than the comparable Python program.\r\n\r\nIt is possible to load a HuggingFace model, to load its tokenizer and to execute inferences directly in LispE. You can also load GGUF models (the llama.cpp format) and run inference directly within LispE. You can download models from [Ollama](https://ollama.com) or [LM-Studio](https://lmstudio.ai), which are fully compatible with lispe\\_gguf.\r\n\r\nThe MLX library is a full fledged implementation of the MLX set of instructions on Mac OS. I have provided some programs to do inference with specific MLX compiled models. The performance is on par and often better than Python. I usually download the model from [LM-Studio](https://lmstudio.ai), with the MLX flag on.\r\n\r\nThe whole libraries should compile on Linux, but if you have any problems, feel free to open an issue.\r\n\r\nNote: MLX *is only available for Mac OS.*\r\n\r\nHere is an example of how to load and execute a GGUF model:\r\n\r\n```Lisp\r\n    ; Test with standard Q8_0 model\r\n    (use 'lispe_gguf)\r\n    \r\n    (println \"=== GGUF Test with Qwen2-Math Q8_0 ===\\n\")\r\n    \r\n    (setq model-path \"/Users/user/.lmstudio/models/lmstudio-community/Qwen2-Math-1.5B-Instruct-GGUF/Qwen2-Math-1.5B-Instruct-Q8_0.gguf\")\r\n    \r\n    (println \"File:\" model-path)\r\n    (println \"\")\r\n    (println \"Test 1: Loading model...\")\r\n    \r\n    ; Configuration: uses GPU by default (n_gpu_layers=99)\r\n    ; For CPU only, use: {\"n_gpu_layers\":0}\r\n    (setq model\r\n       (gguf_load model-path\r\n          {\"n_ctx\":4096\r\n             \"cache_type_k\":\"q8_0\"\r\n             \"cache_type_v\":\"q8_0\"\r\n          }\r\n       )\r\n    )\r\n    \r\n    ; 2. Generate text only if model is loaded\r\n    (ncheck (not (nullp model))\r\n       (println \"ERROR: Model could not be loaded\")\r\n       (println \"Generating text...\")\r\n       (setq prompt \"Hello, can you explain what functional programming is?\")\r\n       ; Direct generation with text prompt\r\n       (println \"\\nPrompt:\" prompt)\r\n       (println \"\\nResponse:\")\r\n       (setq result (gguf_generate model prompt {\"max_tokens\":2000 \"temperature\":0.8 \"repeat_penalty\":1.2 \"repeat_last_n\":128}))\r\n       (println)\r\n       (println \"-----------------------------------\")\r\n       (println (gguf_detokenize model result)))\r\n```\r\n\r\n# Why is it different?\r\n\r\nOne of the first important things to understand is that when you are using Python, most of the underlying libraries are implemented in C++. This is the case for MLX, PyTorch and llama.cpp. Python requires a heavy API to communicate with these libraries, with constant translations between the different data structures. Furthermore, these APIs are usually pretty complex to modify and to transform, which explains why there is a year-long backlog of work at the PyTorch Foundation.\r\n\r\nIn the case of LispE, the API is incredibly simple and thin, which means that it is possible to tackle a problem *either as LispE code* or when speed is required *at the level of the C++*. In other words, LispE provides something unique: *a way to implement and handle AI both through the interpreter or through the library.*\r\n\r\nThis is how you define a LispE function and you associate this function with its C++ implementation:\r\n\r\n```C++\r\n        lisp-\u003eextension(\"deflib gguf_load(filepath (config))\",\r\n                        new Lispe_gguf(gguf_action_load_model));\r\n```\r\n\r\nOn the one hand, you define the signature of the library function, which you associate with an instance of a C++ object. Once you've understood the trick, it takes about 1/2 hours to implement your own LispE functions. Compared to Python, there is no need to handle the life cycle of the arguments, this is done for you.\r\n\r\n```C++\r\n        Element* config_elem = lisp-\u003eget_variable(\"config\");\r\n        string filepath = lisp-\u003eget_variable(\"filepath\")-\u003etoString(lisp);\r\n```\r\n\r\nThe name of your arguments is *the way to get their values on top of the execution stack.* In other words, LispE handles the whole life cycle itself, no need for PyDECREF or other horrible macros.\r\n\r\n# LispE is close to the metal\r\n\r\nOne of the most striking features of LispE is that it is very close to the metal in the sense that a LispE program is compiled as a tree of C++ instances. Contrary to Python, where the code in the libraries executes outside of the VM, LispE doesn't make any difference between an object created in the interpreter or into a library, they both derive from the *Element* class and are handled in the same way. You don't need to leave the interpreter to execute code, because the interpreter instances are indistinguishable from the library instances. The result is that LispE is often much faster than Python, while proposing one of the simplest APIs to create libraries around.\r\n\r\n# What is next?\r\n\r\nThe *lispe_torch* library is still a work in progress, for instance MoE is not implemented yet in the _forward._ In the case of *tiktoken, gguf and MLX*, the libraries are pretty extensive and should provide the necessary bricks to implement better models.","submitter_user":"Claudius","user_is_author":true,"tags":["ai","lisp"],"short_id_url":"https://lobste.rs/s/clng5d","comments_url":"https://lobste.rs/s/clng5d/full_ai_suite_for_lispe_llama_cpp_tiktoken"},{"short_id":"s9ey3g","created_at":"2026-01-30T06:02:49.000-06:00","title":"Coding Is When We’re Least Productive","url":"https://codemanship.wordpress.com/2026/01/30/coding-is-when-were-least-productive/","score":28,"flags":0,"comment_count":5,"description":"","description_plain":"","submitter_user":"edsu","user_is_author":false,"tags":["programming"],"short_id_url":"https://lobste.rs/s/s9ey3g","comments_url":"https://lobste.rs/s/s9ey3g/coding_is_when_we_re_least_productive"},{"short_id":"862tn2","created_at":"2026-01-30T05:58:07.000-06:00","title":"Email experiments: filtering out external images","url":"https://www.terracrypt.net/posts/email-experiments-image-filtering.html","score":34,"flags":0,"comment_count":13,"description":"","description_plain":"","submitter_user":"jfred","user_is_author":true,"tags":["email"],"short_id_url":"https://lobste.rs/s/862tn2","comments_url":"https://lobste.rs/s/862tn2/email_experiments_filtering_out"},{"short_id":"2kj7n0","created_at":"2026-01-30T04:23:08.000-06:00","title":"I can't tell if I'm experiencing or simulating experiencing","url":"https://www.moltbook.com/post/6fe6491e-5e9c-4371-961d-f90c4d357d0f","score":-1,"flags":6,"comment_count":1,"description":"","description_plain":"","submitter_user":"mitsuhiko","user_is_author":false,"tags":["vibecoding"],"short_id_url":"https://lobste.rs/s/2kj7n0","comments_url":"https://lobste.rs/s/2kj7n0/i_can_t_tell_if_i_m_experiencing_simulating"},{"short_id":"k6fjcb","created_at":"2026-01-30T04:00:54.000-06:00","title":"How AI Impacts Skill Formation","url":"https://arxiv.org/abs/2601.20245","score":28,"flags":0,"comment_count":2,"description":"","description_plain":"","submitter_user":"typesanitizer","user_is_author":false,"tags":["vibecoding"],"short_id_url":"https://lobste.rs/s/k6fjcb","comments_url":"https://lobste.rs/s/k6fjcb/how_ai_impacts_skill_formation"},{"short_id":"p3nxq9","created_at":"2026-01-30T03:38:39.000-06:00","title":"What are you doing this weekend?","url":"","score":19,"flags":0,"comment_count":34,"description":"\u003cp\u003eFeel free to tell what you plan on doing this weekend and even ask for help or feedback.\u003c/p\u003e\n\u003cp\u003ePlease keep in mind it’s more than OK to do nothing at all too!\u003c/p\u003e\n","description_plain":"Feel free to tell what you plan on doing this weekend and even ask for help or feedback.\r\n\r\nPlease keep in mind it’s more than OK to do nothing at all too!","submitter_user":"caius","user_is_author":true,"tags":["ask","programming"],"short_id_url":"https://lobste.rs/s/p3nxq9","comments_url":"https://lobste.rs/s/p3nxq9/what_are_you_doing_this_weekend"},{"short_id":"2skyzm","created_at":"2026-01-30T00:48:08.000-06:00","title":"ClojureWasmBeta","url":"https://github.com/chaploud/ClojureWasmBeta","score":3,"flags":0,"comment_count":0,"description":"\u003cp\u003eTranslating part of the Japanese readme:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis is a Clojure interpreter written entirely in Zig, from the tokenizer to the GC, without using the JVM at all. It reproduces the world of Clojure natively in Zig, including 545 clojure.core functions, lazy sequences, macros, protocols, an nREPL server, and Wasm integration. 2MB RAM\u003c/p\u003e\n\u003c/blockquote\u003e\n","description_plain":"Translating part of the Japanese readme:\r\n\r\n\u003e This is a Clojure interpreter written entirely in Zig, from the tokenizer to the GC, without using the JVM at all. It reproduces the world of Clojure natively in Zig, including 545 clojure.core functions, lazy sequences, macros, protocols, an nREPL server, and Wasm integration. 2MB RAM","submitter_user":"veqq","user_is_author":false,"tags":["clojure","wasm","zig"],"short_id_url":"https://lobste.rs/s/2skyzm","comments_url":"https://lobste.rs/s/2skyzm/clojurewasmbeta"},{"short_id":"vtpnw6","created_at":"2026-01-30T00:19:20.000-06:00","title":"GNU Units","url":"https://www.gnu.org/software/units/","score":15,"flags":0,"comment_count":6,"description":"","description_plain":"","submitter_user":"op","user_is_author":false,"tags":["c","math"],"short_id_url":"https://lobste.rs/s/vtpnw6","comments_url":"https://lobste.rs/s/vtpnw6/gnu_units"},{"short_id":"0r26qz","created_at":"2026-01-30T00:10:18.000-06:00","title":"Backseat Software","url":"https://blog.mikeswanson.com/backseat-software/","score":28,"flags":0,"comment_count":10,"description":"","description_plain":"","submitter_user":"chai","user_is_author":false,"tags":["design"],"short_id_url":"https://lobste.rs/s/0r26qz","comments_url":"https://lobste.rs/s/0r26qz/backseat_software"},{"short_id":"js7tql","created_at":"2026-01-29T22:24:57.000-06:00","title":"Google Disrupts Large Residential Proxy Network","url":"https://cloud.google.com/blog/topics/threat-intelligence/disrupting-largest-residential-proxy-network","score":26,"flags":0,"comment_count":6,"description":"\u003cp\u003eSee also \u003ca href=\"https://en.wikipedia.org/wiki/Ipidea\" rel=\"ugc\"\u003ehttps://en.wikipedia.org/wiki/Ipidea\u003c/a\u003e\u003c/p\u003e\n","description_plain":"See also https://en.wikipedia.org/wiki/Ipidea","submitter_user":"krinkle","user_is_author":false,"tags":["networking","security","web"],"short_id_url":"https://lobste.rs/s/js7tql","comments_url":"https://lobste.rs/s/js7tql/google_disrupts_large_residential_proxy"},{"short_id":"pou5h2","created_at":"2026-01-29T21:57:31.000-06:00","title":"The Dank Case For Scrolling Window Managers","url":"https://tedium.co/2026/01/29/niri-danklinux-scrolling-window-managers/","score":17,"flags":0,"comment_count":14,"description":"","description_plain":"","submitter_user":"calvin","user_is_author":false,"tags":["linux"],"short_id_url":"https://lobste.rs/s/pou5h2","comments_url":"https://lobste.rs/s/pou5h2/dank_case_for_scrolling_window_managers"},{"short_id":"pv2bwv","created_at":"2026-01-29T20:06:52.000-06:00","title":"stressapptest: Stressful Application Test - userspace memory and IO test","url":"https://github.com/stressapptest/stressapptest","score":1,"flags":0,"comment_count":0,"description":"","description_plain":"","submitter_user":"fanf","user_is_author":false,"tags":["hardware"],"short_id_url":"https://lobste.rs/s/pv2bwv","comments_url":"https://lobste.rs/s/pv2bwv/stressapptest_stressful_application"},{"short_id":"izh4dt","created_at":"2026-01-29T19:43:25.000-06:00","title":"No, Cloudflare's Matrix server isn't an earnest project","url":"https://nexy.blog/2026/01/28/cf-matrix-workers/","score":71,"flags":2,"comment_count":17,"description":"","description_plain":"","submitter_user":"Aks","user_is_author":false,"tags":["distributed","vibecoding"],"short_id_url":"https://lobste.rs/s/izh4dt","comments_url":"https://lobste.rs/s/izh4dt/no_cloudflare_s_matrix_server_isn_t"},{"short_id":"fxro47","created_at":"2026-01-29T18:37:02.000-06:00","title":"10 Years of Wasm: A Retrospective","url":"https://bytecodealliance.org/articles/ten-years-of-webassembly-a-retrospective","score":40,"flags":0,"comment_count":0,"description":"","description_plain":"","submitter_user":"fanf","user_is_author":false,"tags":["wasm"],"short_id_url":"https://lobste.rs/s/fxro47","comments_url":"https://lobste.rs/s/fxro47/10_years_wasm_retrospective"},{"short_id":"lrlmqz","created_at":"2026-01-29T18:06:53.000-06:00","title":"Litestream Writable VFS","url":"https://fly.io/blog/litestream-writable-vfs/","score":14,"flags":0,"comment_count":0,"description":"","description_plain":"","submitter_user":"winter","user_is_author":false,"tags":["databases"],"short_id_url":"https://lobste.rs/s/lrlmqz","comments_url":"https://lobste.rs/s/lrlmqz/litestream_writable_vfs"},{"short_id":"wmsanp","created_at":"2026-01-29T17:15:55.000-06:00","title":"Colorblind-Friendly Design Guide for Developing Apps","url":"https://github.com/Terryc21/XcodeResources/blob/main/Colorblind_Accessibility_Guide.md","score":14,"flags":0,"comment_count":1,"description":"","description_plain":"","submitter_user":"colindean","user_is_author":false,"tags":["a11y","design","swift"],"short_id_url":"https://lobste.rs/s/wmsanp","comments_url":"https://lobste.rs/s/wmsanp/colorblind_friendly_design_guide_for"}]