[{"short_id":"n54hg3","created_at":"2026-02-25T10:41:12.000-06:00","title":"Software Engineering Has Changed","url":"https://open.substack.com/pub/lukesnotebook/p/software-engineering-has-changed?r=g5e5f\u0026showWelcomeOnShare=true","score":1,"flags":1,"comment_count":0,"description":"","description_plain":"","submitter_user":"lukewilson","user_is_author":true,"tags":["vibecoding"],"short_id_url":"https://lobste.rs/s/n54hg3","comments_url":"https://lobste.rs/s/n54hg3/software_engineering_has_changed"},{"short_id":"ocjdrt","created_at":"2026-02-25T10:41:09.000-06:00","title":"Dictionary of Algorithms and Data Structures","url":"https://xlinux.nist.gov/dads/","score":1,"flags":0,"comment_count":0,"description":"","description_plain":"","submitter_user":"regulator","user_is_author":false,"tags":["compsci"],"short_id_url":"https://lobste.rs/s/ocjdrt","comments_url":"https://lobste.rs/s/ocjdrt/dictionary_algorithms_data_structures"},{"short_id":"4zhkwb","created_at":"2026-02-25T09:38:30.000-06:00","title":"ANN: Tada 0.4.0 - local (cached) dependencies support","url":"https://github.com/tomekw/tada/releases/tag/v0.4.0","score":-1,"flags":2,"comment_count":1,"description":"","description_plain":"","submitter_user":"tomekw","user_is_author":true,"tags":["programming","release"],"short_id_url":"https://lobste.rs/s/4zhkwb","comments_url":"https://lobste.rs/s/4zhkwb/ann_tada_0_4_0_local_cached_dependencies"},{"short_id":"10nlgf","created_at":"2026-02-25T08:57:10.000-06:00","title":"How AI Will Change the Mobile Ecosystem","url":"https://blog.bensontech.dev/posts/How-ai-will-change-mobile-development/","score":-1,"flags":3,"comment_count":0,"description":"","description_plain":"","submitter_user":"informal","user_is_author":true,"tags":["mobile","vibecoding"],"short_id_url":"https://lobste.rs/s/10nlgf","comments_url":"https://lobste.rs/s/10nlgf/how_ai_will_change_mobile_ecosystem"},{"short_id":"owreaj","created_at":"2026-02-25T08:45:13.000-06:00","title":"The Road Ahead for LocalStack: Upcoming Changes to the Delivery of Our AWS Cloud Emulators (2025)","url":"https://blog.localstack.cloud/the-road-ahead-for-localstack/","score":1,"flags":0,"comment_count":1,"description":"\u003cp\u003eIt was published in 2025 but the change will take effect in March, and will likely impact audiences who didn't see their post or otherwise forgot about it:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWhat is changing?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003eOur current distribution for our AWS emulator, which was packaged both as a free Community image (\u003ccode\u003elocalstack/localstack\u003c/code\u003e) and a commercial Pro image (\u003ccode\u003elocalstack/localstack-pro\u003c/code\u003e) on Docker Hub, will be consolidated into a single image beginning in March 2026. Going forward, LocalStack for AWS will be available via a single image distribution on Docker Hub at \u003ccode\u003elocalstack/localstack\u003c/code\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe new unified distribution will require authentication via an auth token for use. This means that, once the change is published in March, pulling and running \u003ccode\u003elocalstack/localstack:latest\u003c/code\u003e will prompt you for an auth token if you have not already provided one.\u003c/p\u003e\n\u003c/blockquote\u003e\n","description_plain":"It was published in 2025 but the change will take effect in March, and will likely impact audiences who didn't see their post or otherwise forgot about it:\r\n\r\n\u003e What is changing?\r\n\r\n\u003e Our current distribution for our AWS emulator, which was packaged both as a free Community image (`localstack/localstack`) and a commercial Pro image (`localstack/localstack-pro`) on Docker Hub, will be consolidated into a single image beginning in March 2026. Going forward, LocalStack for AWS will be available via a single image distribution on Docker Hub at `localstack/localstack`.\r\n\r\n\u003e The new unified distribution will require authentication via an auth token for use. This means that, once the change is published in March, pulling and running `localstack/localstack:latest` will prompt you for an auth token if you have not already provided one.","submitter_user":"mdaniel","user_is_author":false,"tags":["devops","testing"],"short_id_url":"https://lobste.rs/s/owreaj","comments_url":"https://lobste.rs/s/owreaj/road_ahead_for_localstack_upcoming"},{"short_id":"bwkwba","created_at":"2026-02-25T08:35:23.000-06:00","title":"New accounts on HN 10x more likely to use EM-dashes","url":"https://www.marginalia.nu/weird-ai-crap/hn/","score":27,"flags":1,"comment_count":20,"description":"","description_plain":"","submitter_user":"mccd","user_is_author":false,"tags":["vibecoding"],"short_id_url":"https://lobste.rs/s/bwkwba","comments_url":"https://lobste.rs/s/bwkwba/new_accounts_on_hn_10x_more_likely_use_em"},{"short_id":"shzh0m","created_at":"2026-02-25T08:26:21.000-06:00","title":"Current Large Audio Language Models largely transcribe rather than listen","url":"https://arxiv.org/abs/2510.10444","score":6,"flags":0,"comment_count":0,"description":"","description_plain":"","submitter_user":"crmne","user_is_author":false,"tags":["ai","audio"],"short_id_url":"https://lobste.rs/s/shzh0m","comments_url":"https://lobste.rs/s/shzh0m/current_large_audio_language_models"},{"short_id":"8utm05","created_at":"2026-02-25T08:23:13.000-06:00","title":"Tests Are The New Moat","url":"https://saewitz.com/tests-are-the-new-moat","score":4,"flags":0,"comment_count":5,"description":"","description_plain":"","submitter_user":"alper","user_is_author":false,"tags":["practices","vibecoding"],"short_id_url":"https://lobste.rs/s/8utm05","comments_url":"https://lobste.rs/s/8utm05/tests_are_new_moat"},{"short_id":"d4lblv","created_at":"2026-02-25T07:50:34.000-06:00","title":"Lambda World 2019 - Language-Oriented Programming with Racket - Matthias Felleisen","url":"https://www.youtube.com/watch?v=z8Pz4bJV3Tk","score":8,"flags":0,"comment_count":0,"description":"","description_plain":"","submitter_user":"b--man","user_is_author":false,"tags":["lisp","programming","video"],"short_id_url":"https://lobste.rs/s/d4lblv","comments_url":"https://lobste.rs/s/d4lblv/lambda_world_2019_language_oriented"},{"short_id":"qjl5xc","created_at":"2026-02-25T07:07:01.000-06:00","title":"Your system is fine. Your users aren't","url":"https://blog.incrementalforgetting.tech/p/your-system-is-fine-your-users-arent","score":5,"flags":3,"comment_count":1,"description":"","description_plain":"","submitter_user":"dunyakirkali","user_is_author":true,"tags":["practices"],"short_id_url":"https://lobste.rs/s/qjl5xc","comments_url":"https://lobste.rs/s/qjl5xc/your_system_is_fine_your_users_aren_t"},{"short_id":"2sdod9","created_at":"2026-02-25T06:51:55.000-06:00","title":"Building Index-Backed Query Plans in DataFusion","url":"https://pierrezemb.fr/posts/datafusion-index-provider/","score":1,"flags":0,"comment_count":0,"description":"","description_plain":"","submitter_user":"emschwartz","user_is_author":false,"tags":["databases"],"short_id_url":"https://lobste.rs/s/2sdod9","comments_url":"https://lobste.rs/s/2sdod9/building_index_backed_query_plans"},{"short_id":"fcnm2x","created_at":"2026-02-25T04:02:06.000-06:00","title":"Mercury: Ultra-Fast Language Models Based on Diffusion","url":"https://arxiv.org/abs/2506.17298","score":3,"flags":0,"comment_count":0,"description":"\u003cp\u003eMachine learning researchers have been locked in the autoregressive bottleneck for years. A recent paper argues that instead, diffusion models can perform at scale on discrete data. The researchers trained two coding models named Mercury Coder Mini and Small. The Mini model reached a staggering 1109 tokens per second on H100 GPUs, with the Small model achieving 737. These models eclipsed competing efficient state-of-the-art models in throughput by factors of up to ten, while retaining their ability to perform the coding tasks they were trained on. On real world testing and human evaluation platforms such as the Copilot Arena, the Mini model tied for second place in quality with massive models like GPT-4o, having an average latency of only 25 milliseconds. The model matched the performance of established high-speed models such as Claude 3.5 Haiku and Gemini 2.0 Flash Lite across a variety of programming languages, but with many orders of magnitude improvement in decode speed.\u003c/p\u003e\n\u003cp\u003eDiffusion models have a clear advantage over older autoregressive ones in their ability to generate text in parallel, which makes things much more efficient. Standard language models are hamstrung by a serial decoding process in which answers have to be produced one piece at a time. Transformer models abandon that bottleneck entirely. They learn to predict many pieces of text all at once. You start with a string of random noise and run a denoting process that refines all the tokens in concert, zooming from coarse to fine, until the final text emerges. This ability to generate in parallel achieves much higher arithmetic intensity and makes full use of the computational power of modern GPUs.\u003c/p\u003e\n","description_plain":"Machine learning researchers have been locked in the autoregressive bottleneck for years. A recent paper argues that instead, diffusion models can perform at scale on discrete data. The researchers trained two coding models named Mercury Coder Mini and Small. The Mini model reached a staggering 1109 tokens per second on H100 GPUs, with the Small model achieving 737. These models eclipsed competing efficient state-of-the-art models in throughput by factors of up to ten, while retaining their ability to perform the coding tasks they were trained on. On real world testing and human evaluation platforms such as the Copilot Arena, the Mini model tied for second place in quality with massive models like GPT-4o, having an average latency of only 25 milliseconds. The model matched the performance of established high-speed models such as Claude 3.5 Haiku and Gemini 2.0 Flash Lite across a variety of programming languages, but with many orders of magnitude improvement in decode speed.\r\n\r\nDiffusion models have a clear advantage over older autoregressive ones in their ability to generate text in parallel, which makes things much more efficient. Standard language models are hamstrung by a serial decoding process in which answers have to be produced one piece at a time. Transformer models abandon that bottleneck entirely. They learn to predict many pieces of text all at once. You start with a string of random noise and run a denoting process that refines all the tokens in concert, zooming from coarse to fine, until the final text emerges. This ability to generate in parallel achieves much higher arithmetic intensity and makes full use of the computational power of modern GPUs.","submitter_user":"Yogthos","user_is_author":false,"tags":["ai"],"short_id_url":"https://lobste.rs/s/fcnm2x","comments_url":"https://lobste.rs/s/fcnm2x/mercury_ultra_fast_language_models_based"},{"short_id":"ei6o4s","created_at":"2026-02-25T00:14:31.000-06:00","title":"security and blobs","url":"https://blog.lx.oliva.nom.br/2026-02-22-security-and-blobs.en.html","score":1,"flags":0,"comment_count":1,"description":"","description_plain":"","submitter_user":"pizzaiolo","user_is_author":false,"tags":["security"],"short_id_url":"https://lobste.rs/s/ei6o4s","comments_url":"https://lobste.rs/s/ei6o4s/security_blobs"},{"short_id":"tajbtl","created_at":"2026-02-24T23:17:16.000-06:00","title":"I Taught My Dog to Vibe Code Games","url":"https://www.calebleak.com/posts/dog-game/","score":1,"flags":0,"comment_count":2,"description":"","description_plain":"","submitter_user":"atharva","user_is_author":false,"tags":["games","vibecoding"],"short_id_url":"https://lobste.rs/s/tajbtl","comments_url":"https://lobste.rs/s/tajbtl/i_taught_my_dog_vibe_code_games"},{"short_id":"tkqy2l","created_at":"2026-02-24T22:57:15.000-06:00","title":"How Predator Spyware Defeats iOS Camera/Microphone Recording Indicators","url":"https://www.jamf.com/blog/predator-spyware-ios-recording-indicator-bypass-analysis/","score":1,"flags":3,"comment_count":2,"description":"","description_plain":"","submitter_user":"snazz","user_is_author":false,"tags":["ios","security"],"short_id_url":"https://lobste.rs/s/tkqy2l","comments_url":"https://lobste.rs/s/tkqy2l/how_predator_spyware_defeats_ios_camera"},{"short_id":"qjdakc","created_at":"2026-02-24T21:26:34.000-06:00","title":"Turing Completeness of GNU find: From mkdir-assisted Loops to Standalone Computation","url":"https://arxiv.org/pdf/2602.20762","score":16,"flags":0,"comment_count":1,"description":"\u003cp\u003eAbstract: The Unix command find is among the first commands taught to beginners, yet remains\nindispensable for experienced engineers. In this paper, we demonstrate that find possesses\nunexpected computational power, establishing three Turing completeness results using the\nGNU implementation (a standard in Linux distributions). (1) find + mkdir (a system that\nhas only find and mkdir) is Turing complete: by encoding computational states as directory\npaths and using regex back-references to copy substrings, we simulate 2-tag systems. (2)\nGNU find 4.9.0+ alone is Turing complete: by reading and writing to files during traver-\nsal, we simulate a two-counter machine without mkdir. (3) find + mkdir without regex\nback-references is still Turing complete: by a trick of encoding regex patterns directly into\ndirectory names, we achieve the same power.\nThese results place find among the “surprisingly Turing-complete” systems, highlighting\nthe hidden complexity within seemingly simple standard utilities.\u003c/p\u003e\n","description_plain":"Abstract: The Unix command find is among the first commands taught to beginners, yet remains\r\nindispensable for experienced engineers. In this paper, we demonstrate that find possesses\r\nunexpected computational power, establishing three Turing completeness results using the\r\nGNU implementation (a standard in Linux distributions). (1) find + mkdir (a system that\r\nhas only find and mkdir) is Turing complete: by encoding computational states as directory\r\npaths and using regex back-references to copy substrings, we simulate 2-tag systems. (2)\r\nGNU find 4.9.0+ alone is Turing complete: by reading and writing to files during traver-\r\nsal, we simulate a two-counter machine without mkdir. (3) find + mkdir without regex\r\nback-references is still Turing complete: by a trick of encoding regex patterns directly into\r\ndirectory names, we achieve the same power.\r\nThese results place find among the “surprisingly Turing-complete” systems, highlighting\r\nthe hidden complexity within seemingly simple standard utilities.","submitter_user":"asb","user_is_author":false,"tags":["compsci","pdf"],"short_id_url":"https://lobste.rs/s/qjdakc","comments_url":"https://lobste.rs/s/qjdakc/turing_completeness_gnu_find_from_mkdir"},{"short_id":"z8w0ke","created_at":"2026-02-24T20:50:10.000-06:00","title":"LeanServer WASM — Verified Cryptography in the Browser","url":"https://github.com/AfonsoBitoque/LeanServerWASM","score":2,"flags":0,"comment_count":0,"description":"","description_plain":"","submitter_user":"regulator","user_is_author":false,"tags":["cryptography","formalmethods","wasm"],"short_id_url":"https://lobste.rs/s/z8w0ke","comments_url":"https://lobste.rs/s/z8w0ke/leanserver_wasm_verified_cryptography"},{"short_id":"n0cjbj","created_at":"2026-02-24T20:12:32.000-06:00","title":"Build Your Own Forth Interpreter | Coding Challenges","url":"https://codingchallenges.fyi/challenges/challenge-forth","score":1,"flags":0,"comment_count":0,"description":"","description_plain":"","submitter_user":"spc476","user_is_author":false,"tags":["concatenative"],"short_id_url":"https://lobste.rs/s/n0cjbj","comments_url":"https://lobste.rs/s/n0cjbj/build_your_own_forth_interpreter_coding"},{"short_id":"mo0hgf","created_at":"2026-02-24T19:43:23.000-06:00","title":"Justifying text-wrap: pretty","url":"https://matklad.github.io/2026/02/14/justifying-text-wrap-pretty.html","score":38,"flags":0,"comment_count":9,"description":"","description_plain":"","submitter_user":"vulcan","user_is_author":false,"tags":["browsers","css","design","web"],"short_id_url":"https://lobste.rs/s/mo0hgf","comments_url":"https://lobste.rs/s/mo0hgf/justifying_text_wrap_pretty"},{"short_id":"rhoktj","created_at":"2026-02-24T16:59:23.000-06:00","title":"ansigpt: c89 implementation of microgpt","url":"https://github.com/yobibyte/ansigpt","score":10,"flags":0,"comment_count":6,"description":"","description_plain":"","submitter_user":"y0b1byte","user_is_author":true,"tags":["ai","c"],"short_id_url":"https://lobste.rs/s/rhoktj","comments_url":"https://lobste.rs/s/rhoktj/ansigpt_c89_implementation_microgpt"},{"short_id":"xkzo35","created_at":"2026-02-24T16:43:07.000-06:00","title":"RFC 406i - The Rejection of Artificially Generated Slop (RAGS)","url":"https://406.fail/","score":45,"flags":0,"comment_count":11,"description":"","description_plain":"","submitter_user":"olliej","user_is_author":false,"tags":["satire","vibecoding"],"short_id_url":"https://lobste.rs/s/xkzo35","comments_url":"https://lobste.rs/s/xkzo35/rfc_406i_rejection_artificially"},{"short_id":"uqu9qv","created_at":"2026-02-24T16:40:32.000-06:00","title":"Parse Me, Baby, One More Time: Bypassing HTML Sanitizer via Parsing Differentials (2024)","url":"https://www.ias.cs.tu-bs.de/publications/parsing_differentials.pdf","score":1,"flags":0,"comment_count":0,"description":"","description_plain":"","submitter_user":"fanf","user_is_author":false,"tags":["pdf","security","web"],"short_id_url":"https://lobste.rs/s/uqu9qv","comments_url":"https://lobste.rs/s/uqu9qv/parse_me_baby_one_more_time_bypassing_html"},{"short_id":"fjrzzm","created_at":"2026-02-24T16:31:58.000-06:00","title":"How we rebuilt Next.js with AI in one week","url":"https://blog.cloudflare.com/vinext/","score":16,"flags":3,"comment_count":10,"description":"","description_plain":"","submitter_user":"Joban","user_is_author":false,"tags":["vibecoding"],"short_id_url":"https://lobste.rs/s/fjrzzm","comments_url":"https://lobste.rs/s/fjrzzm/how_we_rebuilt_next_js_with_ai_one_week"},{"short_id":"beirlw","created_at":"2026-02-24T16:20:39.000-06:00","title":"Dear researchers: Is AI all you've got?","url":"https://austinhenley.com/blog/dearresearchers.html","score":7,"flags":0,"comment_count":0,"description":"","description_plain":"","submitter_user":"jryans","user_is_author":false,"tags":["ai","culture"],"short_id_url":"https://lobste.rs/s/beirlw","comments_url":"https://lobste.rs/s/beirlw/dear_researchers_is_ai_all_you_ve_got"},{"short_id":"iaukur","created_at":"2026-02-24T16:14:12.000-06:00","title":"Producing garbage for the AI","url":"https://alexschroeder.ch/view/2026-02-20-garbage","score":6,"flags":0,"comment_count":1,"description":"","description_plain":"","submitter_user":"lr0","user_is_author":false,"tags":["ai","web"],"short_id_url":"https://lobste.rs/s/iaukur","comments_url":"https://lobste.rs/s/iaukur/producing_garbage_for_ai"}]